{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "merge_clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf78a90d2ece47739ff8d1862531ed9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82d8c8f645504a18bdd6417f44eeb49d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6c49255bf47e4a6fafb325bff844fe08",
              "IPY_MODEL_aa79e88c4e684ed0945e2302033ad2f8"
            ]
          }
        },
        "82d8c8f645504a18bdd6417f44eeb49d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c49255bf47e4a6fafb325bff844fe08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8ef1e8eed2b0429fababbb6ecd28d0ee",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 48,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 48,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1e85db72de84dc6809a1c53202de67b"
          }
        },
        "aa79e88c4e684ed0945e2302033ad2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2fb6eb94bf44463fa1f9690a66b60ab4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 48/48 [00:55&lt;00:00,  1.15s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78356d630ee144c3be034173906b2ede"
          }
        },
        "8ef1e8eed2b0429fababbb6ecd28d0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1e85db72de84dc6809a1c53202de67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fb6eb94bf44463fa1f9690a66b60ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78356d630ee144c3be034173906b2ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58689217b42042c6bc9379eb295adfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4b60c06c346340a982122652b5bf7bdf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d18f89c4eca460a8d33e91c8bf38422",
              "IPY_MODEL_85b7c2c7c9a64254a419926f276f2942"
            ]
          }
        },
        "4b60c06c346340a982122652b5bf7bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d18f89c4eca460a8d33e91c8bf38422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_335d175ef5b74df1888cc629133d4a52",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 613949,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 613949,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7db38de482854e8ebe67083a9c02f9aa"
          }
        },
        "85b7c2c7c9a64254a419926f276f2942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5482913584884a7b927301bed5c6437d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 613949/613949 [15:58&lt;00:00, 640.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da943f29fe684f33af8b9013780410a2"
          }
        },
        "335d175ef5b74df1888cc629133d4a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7db38de482854e8ebe67083a9c02f9aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5482913584884a7b927301bed5c6437d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da943f29fe684f33af8b9013780410a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZTh_8KUlT9b"
      },
      "source": [
        "# Import Packages and Mount Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtfjwrLx60jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb912349-cca1-4a66-e2ac-fa62072c582a"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfcBaeYalYzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59e80a4-134a-4fe7-d967-4dbcff02a497"
      },
      "source": [
        "import glob, json, zipfile, os, csv, re, nltk\n",
        "\n",
        "import pandas as pd\n",
        "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import copy as cp\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet') \n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from tqdm.notebook import tqdm as tq\n",
        "tq.pandas()\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel, LdaMulticore\n",
        "from gensim.utils import simple_preprocess \n",
        "import gensim.corpora as corpora\n",
        "from google.colab import files\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB-a9K0rDm9v"
      },
      "source": [
        "Set path to local directory "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16bPH5RPgBDy"
      },
      "source": [
        "## Initalize Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuO8DiCExPmj"
      },
      "source": [
        "CHECKPOINT_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /results/2020Mar11-2021June16/checkpoints'\n",
        "BATCH_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /results/2020Mar11-2021June16/batched_data'\n",
        "DATA_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /Preprocessing/data_to_process/temp'\n",
        "KEYWORDS_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /Preprocessing/COVID_keywords.txt'\n",
        "RESULTS_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /results/2020Mar11-2020June16'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7RdjXGnAblW"
      },
      "source": [
        "## Initialize Checkpoint I/O"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMk7l8jnzLGS"
      },
      "source": [
        "\n",
        "def write_json(filename, df, path):\n",
        "  \n",
        "  with open(path + filename, 'w') as f: \n",
        "      result = df.to_json(orient = 'records', double_precision = 15)\n",
        "      parsed = json.loads(result)\n",
        "      f.write(json.dumps(parsed))\n",
        "  f.close()\n",
        "\n",
        "def read_json(filename, path):\n",
        "  \n",
        "  with open(path + '/'+ filename, 'r') as f: \n",
        "    read = pd.read_json(f, \n",
        "                        orient='records', keep_default_dates=False, precise_float=True) \n",
        "  return pd.DataFrame(read)\n",
        "\n",
        "def read_batch(path):\n",
        "\n",
        "  with open(path, 'r') as f: \n",
        "    read = pd.read_json(f, \n",
        "                        orient='records', keep_default_dates=False, precise_float=True) \n",
        "  return pd.DataFrame(read)\n",
        "\n",
        "def write_csv(filename, lst, path):\n",
        "\n",
        "  with open(path + '/' + filename, 'a') as f:\n",
        "\n",
        "      write = csv.writer(f)\n",
        "      write.writerows(lst)\n",
        "\n",
        "def write_dict(filename, dct, path):\n",
        "  \n",
        "  with open(path + '/' + filename, 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for k,v in dct.items():\n",
        "      writer.writerow([k,v])\n",
        "\n",
        "def read_dict(filename, path):\n",
        "  d = {}\n",
        "  with open(path + '/' + filename) as f:\n",
        "    reader = csv.reader(f)\n",
        "    for k,v in reader: \n",
        "      v = v.strip('][').split(', ')\n",
        "      for i in range(len(v)):\n",
        "        v[i] = v[i].strip('\"')\n",
        "      d[k] = v\n",
        "\n",
        "  return d\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AB40M0UwZvk"
      },
      "source": [
        "#Load Batched Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8trTHhJYOqTn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "cf78a90d2ece47739ff8d1862531ed9b",
            "82d8c8f645504a18bdd6417f44eeb49d",
            "6c49255bf47e4a6fafb325bff844fe08",
            "aa79e88c4e684ed0945e2302033ad2f8",
            "8ef1e8eed2b0429fababbb6ecd28d0ee",
            "e1e85db72de84dc6809a1c53202de67b",
            "2fb6eb94bf44463fa1f9690a66b60ab4",
            "78356d630ee144c3be034173906b2ede"
          ]
        },
        "outputId": "8a995678-4105-4634-8e55-d132d0f3b928"
      },
      "source": [
        "pd.set_option('precision', 30)\n",
        "# Set dates to batch \n",
        "batches_to_merge = sorted(list(glob.iglob(BATCH_PATH + '/*.json')))\n",
        "\n",
        "frames = []\n",
        "total0 = len(batches_to_merge)\n",
        "pbar0 = tq(total=total0, position = 0, leave = True)\n",
        "for batch in batches_to_merge:\n",
        "  filename = 'connected_tweets_' + date + '.json'\n",
        "  df = read_batch(batch)\n",
        "  frames.append(df)\n",
        "  pbar0.update(1)\n",
        "pbar0.close()\n",
        "connected_df = pd.concat(frames)\n",
        "print(connected_df.shape)\n",
        "connected_df.reset_index(inplace=True)\n",
        "connected_df = connected_df.drop_duplicates(subset=['tweet-id'])\n",
        "connected_df = connected_df.where(pd.notnull(connected_df), None)\n",
        "\n",
        "print(connected_df.shape)\n",
        "\n",
        "print(\"Successfully loaded all batched data into dataframe.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf78a90d2ece47739ff8d1862531ed9b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=48.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(631460, 22)\n",
            "(613949, 23)\n",
            "Successfully loaded all batched data into dataframe.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q9QxgcbkWlj"
      },
      "source": [
        "out_of_batch_replies = read_dict('out_of_batch_replies_20210616.csv', CHECKPOINT_PATH)\n",
        "out_of_batch_quotes = read_dict('out_of_batch_quotes_20210616.csv', CHECKPOINT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7KHx9S_lBh4"
      },
      "source": [
        "for key in out_of_batch_replies.keys():\n",
        "  index = connected_df.index[connected_df['tweet-id'] == int(key)].to_list()\n",
        "  if index: \n",
        "    i = index[0]\n",
        "    cleaned_keys = []\n",
        "    for id in out_of_batch_replies[key]:\n",
        "      id = id.strip(\"[']\")\n",
        "      cleaned_keys.append(id)    \n",
        "    connected_df.at[i,'replies'] = connected_df.at[i, 'replies'] = [*connected_df.at[i,'replies'], *cleaned_keys]\n",
        "    \n",
        "for key in out_of_batch_quotes.keys():\n",
        "  index = connected_df.index[connected_df['tweet-id'] == int(key)].to_list()\n",
        "  if index: \n",
        "    i = index[0]\n",
        "    cleaned_keys = []\n",
        "    for id in out_of_batch_quotes[key]:\n",
        "      id = id.strip(\"[']\")\n",
        "      cleaned_keys.append(id)    \n",
        "    connected_df.at[i,'quotes'] = connected_df.at[i, 'quotes'] = [*connected_df.at[i,'quotes'], *cleaned_keys]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZU3LrlsoH3J"
      },
      "source": [
        "# Preprocess Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-z2KdYqFziC"
      },
      "source": [
        "**Steps of preprocessing:**\n",
        "\n",
        "1.   Remove non-alphanumeric characters\n",
        "2.   Remove user mentions and urls\n",
        "3.   Remove # from hashtags\n",
        "4.   Tag parts of speech and lemmatize \n",
        "5.   Tokenize text \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBe-OvU4wwwU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "58689217b42042c6bc9379eb295adfda",
            "4b60c06c346340a982122652b5bf7bdf",
            "3d18f89c4eca460a8d33e91c8bf38422",
            "85b7c2c7c9a64254a419926f276f2942",
            "335d175ef5b74df1888cc629133d4a52",
            "7db38de482854e8ebe67083a9c02f9aa",
            "5482913584884a7b927301bed5c6437d",
            "da943f29fe684f33af8b9013780410a2"
          ]
        },
        "outputId": "6cf74508-1195-4cfe-9a07-05922dc0497e"
      },
      "source": [
        "# initialize wordset\n",
        "WORDSET = set()\n",
        "# download stopwords list from nltk library\n",
        "stop_words_en = stopwords.words('english')\n",
        "stop_words_spa = stopwords.words('spanish')\n",
        "# manually extend stopwards list\n",
        "stop_words_en.extend([\"i'm\", \"\\n\", \"amp\", \"it's\", \"it’s\", \"im\", \"via\", \"like\", \"day\", \n",
        "                  \"time\", \"go\", \"got\", \"get\", \"one\", \"lol\", \"y'all\", \"y’all\", \"gonna\", \"fuck\", \n",
        "                  \"shit\", \"pls\", \"plz\", \"tbh\", \"rn\", \"tbt\", \"ngl\", \"idc\", \"idk\", \"imy\", \n",
        "                    \"i'll\", \"i've\", \"let's\", \"jk\", \"cuz\", \"fucking\", \"fuckin\" \"wow\", \"that's\", \n",
        "                   \"dawg\", \"drop\", \"pin\", \"i'd\", \"lmao\", \"wtf\", \"link\", \"bio\"])\n",
        "\n",
        "\"\"\"\n",
        "Helper function that removes emoji characters from a given text. \n",
        "\n",
        "Parameters: \n",
        "  string -- string of text \n",
        "Returns: \n",
        "  demoji_string -- text without emoji characters\n",
        "\n",
        "\"\"\"\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        " \n",
        "    demoji_string = emoji_pattern.sub(r'', str(string))\n",
        "    return demoji_string\n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "Helper function that transforms part of speech tag to wordnet tag. \n",
        "\n",
        "Parameters: \n",
        "  nltk_tag -- nltk part of speech tag\n",
        "Returns: \n",
        "  corresponding wordnet tag\n",
        "\"\"\"\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "\"\"\"\n",
        "Cleans the text of the tweet by removing non-alphanumeric characters and \n",
        "stop words, and lemmatizing and tokenizing text. \n",
        "\n",
        "Parameters: \n",
        "  text -- string of tweet text \n",
        "\n",
        "Returns: \n",
        "  new_text -- cleaned and tokenized text\n",
        "\"\"\"\n",
        "def clean_text(text):\n",
        "  \n",
        "  # Remove emojis\n",
        "  text = remove_emoji(text)\n",
        "  # Remove urls\n",
        "  text = re.sub(r\"https\\S+\", \"\", text)\n",
        "  # Remove mentions\n",
        "  text = re.sub(r\"@\\S+\", \"\", text)\n",
        "  # Remove hashtags\n",
        "  text = re.sub(r\"#\\S+\", \"\", text)\n",
        "  # Remove numbers\n",
        "  text = re.sub(r\"[0-9\\-]\", \"\", text)\n",
        "  # Remove punctuation \n",
        "  text = re.sub('[,\\/.!?:;_#\\-\\\"\\“@&%*($)+]', \"\",text)\n",
        "  text = re.sub('\\’',\"\\'\", text)\n",
        "  # Lower case letters \n",
        "  text = text.lower()\n",
        "\n",
        "  # Part of speech tagging\n",
        "  pos_tag = nltk.pos_tag(text.split())\n",
        "  wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), pos_tag)\n",
        "  # Lemmatize\n",
        "  lemmatizer = WordNetLemmatizer() \n",
        "  #stemmer = SnowballStemmer(\"english\") \n",
        "  lemmatized_words = []\n",
        "  \n",
        "  for word, tag in wordnet_tagged:\n",
        "      if tag is None: \n",
        "        lword = lemmatizer.lemmatize(word)\n",
        "      else:\n",
        "        lword = lemmatizer.lemmatize(word, pos=tag)\n",
        "      #sword = stemmer.stem(lword)\n",
        "      if (lword not in stop_words_en) and (lword not in stop_words_spa) and (len(lword) > 3):\n",
        "        lemmatized_words.append(lword)\n",
        "        WORDSET.add(lword)\n",
        "      \n",
        "\n",
        "  new_text = lemmatized_words\n",
        "  \n",
        "  return new_text\n",
        "\n",
        "connected_df_dict = connected_df.to_dict(orient = 'records')\n",
        "total = len(connected_df_dict)\n",
        "pbar2 = tq(total=total, position = 0, leave = True)\n",
        "\n",
        "cleaned_tweets = cp.copy(connected_df_dict)\n",
        "for tweet in cleaned_tweets:\n",
        "  tweet['cleaned_text'] = clean_text(cp.copy(tweet['text']))\n",
        "  pbar2.update(1)\n",
        "\n",
        "\n",
        "pbar2.close() \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58689217b42042c6bc9379eb295adfda",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=613949.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Qn9ZwRkzme"
      },
      "source": [
        "# Create Pandas Dataframe "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yil5QqmJpdIN"
      },
      "source": [
        "Merge connected tweets files if batched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8T3_zxTki01"
      },
      "source": [
        "\n",
        "# determine dataframe columns \n",
        "columns = ['tweet-id', 'user-id', 'user-screen_name', 'verified', 'text', 'cleaned_text', \n",
        "           'hashtags', 'mentions' , 'in-reply-to', 'quoted', 'timestamp', 'followers-count',\n",
        "           'replies_count', 'quote_count', 'replies','quotes']\n",
        "\n",
        "\n",
        "\n",
        "# Create a DataFrame from `tweets`\n",
        "cleaned_df = pd.DataFrame(cleaned_tweets, columns=columns)\n",
        "cleaned_df = cleaned_df.where(pd.notnull(cleaned_df), None)\n",
        "\n",
        "cleaned_df['hashtags'] = cleaned_df['hashtags'].apply(lambda x: [w.lower() for w in x]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbGsakqF4Nix"
      },
      "source": [
        "# Hashtag and mentions analysis \n",
        "from collections import Counter\n",
        "all_hashtags = []\n",
        "for i,v in cleaned_df['hashtags'].items():\n",
        "  for tag in v:\n",
        "    all_hashtags.append(tag)\n",
        "hashtag_count = Counter(all_hashtags)\n",
        "all_mentions = []\n",
        "for i,v in cleaned_df['mentions'].items():\n",
        "  for id in v:\n",
        "    all_mentions.append(id)\n",
        "mention_count = Counter(all_mentions)\n",
        "\n",
        "\n",
        "def count_occurences(entity, counter, thresh):\n",
        "  new_dict = {}\n",
        "  for e in entity: \n",
        "    count = counter[e]\n",
        "    if count > thresh:\n",
        "      new_dict[e] = count\n",
        "  return new_dict\n",
        "\n",
        "hashtag_thresh = 10\n",
        "mention_thresh = 10\n",
        "cleaned_df['hashtags'] = cleaned_df['hashtags'].apply(lambda x : count_occurences(x, hashtag_count, hashtag_thresh))\n",
        "cleaned_df['mentions'] = cleaned_df['mentions'].apply(lambda x : count_occurences(x, mention_count, mention_thresh))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtlIj-6z3FI4"
      },
      "source": [
        "user_info = ['user-id', 'user-screen_name', 'verified', 'followers-count', \n",
        "             'favorites-count', 'following-count', 'status-count']\n",
        "user_counts = ['tweets-in-dataset', 'num-tweet-in-dataset', 'replies-in-data-set', \n",
        "               'quotes-in-data-set', 'num-replies', 'num-quotes']\n",
        "\n",
        "\n",
        "unique_users = pd.unique(cleaned_df['user-id'])\n",
        "user_array = []\n",
        "for user in unique_users:\n",
        "  user_data = [] \n",
        "  index = connected_df.index[connected_df['user-id'] == user].to_list()[0]\n",
        "  for header in user_info: \n",
        "    attribute = connected_df.at[index, header]\n",
        "    user_data.append(attribute)\n",
        "  user_df = connected_df.loc[connected_df['user-id'] == user]\n",
        "  tweets = user_df['tweet-id'].to_list()\n",
        "  tweet_count = user_df.shape[0]\n",
        "  replies = []\n",
        "  quotes = []\n",
        "  for i, v in user_df.iterrows():\n",
        "    replies = [*replies, *v['replies']] \n",
        "    quotes = [*quotes, *v['quotes']]\n",
        "  num_replies = len(replies)\n",
        "  num_quotes = len(quotes)\n",
        "  user_data = [*user_data, *[tweets, tweet_count, replies, quotes, num_replies, num_quotes]]\n",
        "  user_array.append(user_data)\n",
        "\n",
        "user_df = pd.DataFrame(user_array, columns=[*user_info, *user_counts])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O6ctgRkzdCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12bdf5d2-11ab-48e6-d6bc-1f3a6df385fc"
      },
      "source": [
        "write_json('processed_tweets_20mar11-21june16.json', cleaned_df, RESULTS_PATH)\n",
        "write_json('user_data_20mar11-21june16.json', user_df, RESULTS_PATH)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvbGyUJCKlg"
      },
      "source": [
        "Checkpoint #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpv0wDnZCE1j"
      },
      "source": [
        "write_json('cleaned_tweets.json', cleaned_df, CHECKPOINT_PATH)\n",
        "print(\"Successfully Saved Checkpoint #2.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Ux-Ue9lyVv"
      },
      "source": [
        "# Keyword Extraction using TFIDF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQPeoQoSCbrY"
      },
      "source": [
        "cleaned_df = read_json('cleaned_tweets.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4SVVoVS4f-o"
      },
      "source": [
        "Extracts top $n$ keywords for each tweet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj1aNs1mlH2s"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from operator import itemgetter\n",
        "\n",
        "docs = cp.copy(cleaned_df)['cleaned_text'].to_list()\n",
        "all_text = []\n",
        "for tweet in docs:\n",
        "  for word in tweet:\n",
        "    all_text.append(word)\n",
        "\n",
        "id2word = corpora.Dictionary(docs)\n",
        "# Ceate Corpus\n",
        "texts = docs\n",
        "\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "for i in range(len(texts)):\n",
        "  texts[i] = ' '.join(texts[i])\n",
        "\n",
        "tfidf = vectorizer.fit_transform(texts)\n",
        "\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "tc = tfidf.tocoo()\n",
        "\n",
        "doc_dict = {}\n",
        "for w,d,s in zip(tc.col, tc.row, tc.data):\n",
        "  word_id = w\n",
        "  word = feature_names[word_id]\n",
        "  doc_num = d\n",
        "  score = s \n",
        "  if doc_num in doc_dict.keys():\n",
        "    doc_dict[doc_num][word] = score\n",
        "  else:\n",
        "    doc_dict[doc_num] = {word: score} \n",
        "\n",
        "tweets_df = cp.copy(cleaned_df)\n",
        "tweets_df['keywords'] = tweets_df.apply(lambda x: [], axis=1)\n",
        "\n",
        "\n",
        "N = 3\n",
        "all_keywords = []\n",
        "for doc in doc_dict.keys():\n",
        "  res = dict(sorted(doc_dict[doc].items(), key = itemgetter(1), reverse = True)[:N])\n",
        "\n",
        "  keywords = list(res.keys())\n",
        "\n",
        "  all_keywords.append(keywords)\n",
        "  tweets_df.at[doc, 'keywords']= keywords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0UIPSV7ydcG"
      },
      "source": [
        "Create Checkpoint #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlAo-70ZDMyV"
      },
      "source": [
        "write_json('tweets_keywords.json', tweets_df, CHECKPOINT_PATH)\n",
        "print(\"Successfully Saved Checkpoint #3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWiw6Frkv6gk"
      },
      "source": [
        "# Calculate Sentiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzA4stogWmbm"
      },
      "source": [
        "It first breaks up sentences into clauses (using punctuation and complementizers as separators), and then for each keyword, its score is the average of the vader score of all clauses where it appears."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMZGtj4Gnt_q"
      },
      "source": [
        "def vaderScore(text):\n",
        "  return analyzer.polarity_scores(text)['compound']\n",
        "\n",
        "def posTagging(text):\n",
        "  return nltk.pos_tag(nltk.word_tokenize(text))\n",
        "\n",
        "def avg(list): \n",
        "    return sum(list)/len(list) if len(list)!=0 else 0\n",
        "\n",
        "clauseDelimeters = ['CC','.','!','?',','] # complementizers and punctuations. \n",
        "\n",
        "def sentimentScore(text, aspects):\n",
        "  tokens = posTagging(text)\n",
        "  scores = {word: [] for word in aspects} \n",
        "      # ALT: just sum scores and divide by total number of clauses\n",
        "  \n",
        "  clause = []\n",
        "  flag = False\n",
        "\n",
        "  for (word,pos) in tokens:\n",
        "    # print(pos)\n",
        "    if pos in clauseDelimeters: # updated logic to include punctuation at the end\n",
        "      flag = True\n",
        "    elif flag: # the first word after a delimeter, i.e. first word of new clause\n",
        "      flag = False\n",
        "      # process the clause\n",
        "      clauseText = ' '.join(clause)\n",
        "      # print(clauseText)\n",
        "      clauseScore = vaderScore(clauseText)\n",
        "      for aspect in aspects:\n",
        "        if aspect in clause:\n",
        "          scores[aspect].append(clauseScore)\n",
        "      clause = []\n",
        "    else:\n",
        "      flag = False\n",
        "\n",
        "    # clause.append(word)\n",
        "    if pos != 'CC':\n",
        "      clause.append(word) # exclude complementizers but include punctuations\n",
        "\n",
        "  if len(clause)!=0: # handle the last clause--end of sentence\n",
        "    clauseText = ' '.join(clause)\n",
        "    # print(clauseText)\n",
        "    clauseScore = vaderScore(clauseText)\n",
        "    for aspect in aspects:\n",
        "      if aspect in clause:\n",
        "        scores[aspect].append(clauseScore) \n",
        "    \n",
        "  return {word: avg(scores[word]) for word in aspects}\n",
        "\n",
        "\n",
        "print(\"----------Calculating Vader Sentiment Scores-----------\")\n",
        "tweets_df['sentiment_score'] = tweets_df.progress_apply(lambda x : vaderScore(x['text']), axis=1)\n",
        "print(\"----------Calculating ASBA Sentiment Dictionaries----------\")\n",
        "tweets_df['sentiment_dict'] = tweets_df.progress_apply(lambda x : sentimentScore(x['text'], x['keywords']), axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ5zponSyv6G"
      },
      "source": [
        "Create Checkpoint #4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5McHXlvXMUc_"
      },
      "source": [
        "write_json('sentiment_cp,json', tweets_df, CHECKPOINT_PATH)\n",
        "print(\"Successfully Saved Checkpoint #4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxyFqLnqwJcL"
      },
      "source": [
        "# Topic Modelling\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uJ13UqQz2sc"
      },
      "source": [
        "## Build LDA Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItY4Kau66uz5"
      },
      "source": [
        "Creates corpus in bag-of-words format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhNCjxre6oZy"
      },
      "source": [
        "# Create set of words from all tweets\n",
        "docs = tweets_df['cleaned_text'].to_list()\n",
        "all_text = []\n",
        "for tweet in docs:\n",
        "  for word in tweet:\n",
        "    all_text.append(word)\n",
        "\n",
        "# Create Dictionary \n",
        "id2word = corpora.Dictionary(docs)\n",
        "\n",
        "# Ceate Corpus\n",
        "texts = docs\n",
        "\n",
        "# Term Document Frequency \n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaAf4SOc6qth"
      },
      "source": [
        "Builds latend Dirichlet allocation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCy0PRlKwIMq"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import LdaMulticore, CoherenceModel\n",
        "\n",
        "a = 0.005 # alpha hyperparameter\n",
        "b = 0.81 # beta hyperparameter\n",
        "k = 20 # num_topics hyperparameter\n",
        "\n",
        "lda_model = gensim.models.LdaMulticore(corpus = corpus, \n",
        "                                        id2word=id2word,\n",
        "                                        num_topics = k, \n",
        "                                        chunksize = 1000, \n",
        "                                        random_state = 100,\n",
        "                                        passes = 10, \n",
        "                                        alpha = a, \n",
        "                                        eta = b)\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, \n",
        "                                       dictionary = id2word, coherence = 'c_v' )\n",
        "coherence = coherence_model_lda.get_coherence()\n",
        "print(\"Coherence Score of LDA model:\")\n",
        "print(coherence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1FcaeGtwvdM"
      },
      "source": [
        "## Store LDA Model Results \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz6MKP_SyuHO"
      },
      "source": [
        "Creates new pandas data frame for LDA generated topics. Ouputs keywords generated from model for a particular topic with weightings. Weightings are generated from term-topic matrix learned during inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcjbhyBewQ8Z"
      },
      "source": [
        "results = []\n",
        "topic_keywords = []\n",
        "\n",
        "for i in range(k):\n",
        "  topic_dict = {}\n",
        "  cur_topic = lda_model.show_topic(i)\n",
        "  words = []\n",
        "  weights = []\n",
        "  for tup in cur_topic:\n",
        "    words.append(tup[0])\n",
        "    weights.append(tup[1])\n",
        "  topic_dict['topic_id'] = i\n",
        "  topic_dict['keywords'] = words\n",
        "  topic_dict['weightings'] = weights\n",
        "  topic_dict['keywords_weightings'] = cur_topic\n",
        "  topic_keywords.append([\"Topic \" + str(i), *words])\n",
        "  results.append(topic_dict)\n",
        "\n",
        "\n",
        "write_csv('LDA_topic_keywords.csv', topic_keywords, REULTS_PATH)\n",
        "\n",
        "\n",
        "topic_df = pd.DataFrame(results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dpulAHAz9Wa"
      },
      "source": [
        "## Cluster Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ox3hYSA0DFM"
      },
      "source": [
        "### LDA Topic Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nt6WJKJyv3t"
      },
      "source": [
        "Classifies documents into most probable learned topic based on topic distribution for given document. Also stores the topic distribution for each topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X11J1qD6yemm"
      },
      "source": [
        "# set min_probability threshold \n",
        "thresh = 0.05\n",
        "\n",
        "topic_nums = []\n",
        "topic_dist = []\n",
        "for d in docs:\n",
        "  # transform docuemtn into bow corpus\n",
        "  bow = id2word.doc2bow(d)\n",
        "  # retrieve topic distribution for document\n",
        "  t = lda_model.get_document_topics(bow)\n",
        "  prob = []\n",
        "  for i in t:\n",
        "    prob.append(i[1])\n",
        "  # find topic with highest probability from distribution\n",
        "  topic = prob.index(max(prob))\n",
        "  N = 3\n",
        "  topics = dict(sorted(dict(t).items(), key = itemgetter(1), reverse = True)[:N]).keys()\n",
        "  topic_nums.append(list(topics))\n",
        "  topic_dist.append(t)\n",
        "  \n",
        "\n",
        "\n",
        "tweets_df['LDA_topic'] = topic_nums\n",
        "tweets_df['LDA_distribution'] = topic_dist\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2Pr4JHns9v"
      },
      "source": [
        "print(tweets_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb0_3ncl0WIH"
      },
      "source": [
        "cluster = {}\n",
        "for i in range(20):\n",
        "  cluster[i] = 0\n",
        "for i,v in tweets_df['LDA_topic'].items():\n",
        "  for t in v:\n",
        "    cluster[t] += 1\n",
        "print(cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "691MeVUB3zlm"
      },
      "source": [
        "print(tweets_df['LDA_topic'].head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1f4lYZZ0ODu"
      },
      "source": [
        "### $K$-means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4hInkz50VZ_"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTfjp3ib0Yfy"
      },
      "source": [
        "TFIDF Transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKtJt5r7z6bN"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "for i in range(len(texts)):\n",
        "  texts[i] = ' '.join(texts[i])\n",
        "tfidf = vectorizer.fit_transform(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMcaMf_B0kOk"
      },
      "source": [
        "Build $k$-means model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BNg_AP0n0jMS"
      },
      "source": [
        "kmeans = KMeans(n_clusters=20).fit(tfidf)\n",
        "kmean_predict = kmeans.fit_predict(tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9gRo1D30tvI"
      },
      "source": [
        "Store $k$-means data in topic dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q5OHLbor0u8D"
      },
      "source": [
        "kmean_topics = []\n",
        "for t in kmean_predict:\n",
        "  kmean_topics.append(t)\n",
        "tweets_df['kmeans_topic'] = kmean_topics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsDSGrxg16FX"
      },
      "source": [
        "# Export Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2Jt5PA9186Z"
      },
      "source": [
        "\"\"\"\n",
        "Function that writes dataframe to .json file.\n",
        "\n",
        "Parameters: \n",
        "  filename -- output file name\n",
        "  df -- dataframe to export \n",
        "   \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "output_columns = ['tweet-id', 'text', 'cleaned_text', 'user-id', 'in-reply-to', 'quoted', \n",
        "                  'timestamp', 'followers-count', 'replies', 'quotes ', \n",
        "                  'quote_count', 'replies_count', 'keywords', 'sentiment_dict', \n",
        "                  'sentiment_score', 'LDA_topic', 'LDA_distribution', 'hashtags', 'mentions'] \n",
        "\n",
        "document_df = tweets_df[output_columns]\n",
        "\n",
        "write_json('document_data_test2.json', document_df, RESULTS_PATH)\n",
        "print(\"Document Data Successfully Written to Google Drive.\")\n",
        "write_json('topic_data_test2.json', topic_df, RESULTS_PATH)\n",
        "print(\"Topic Data Successfully Written to Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR7mVXOW8xwS"
      },
      "source": [
        "print(document_df['LDA_topic'].head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRXEUWfa6Kjv"
      },
      "source": [
        "CHECKPOINT_PATH = '/content/drive/MyDrive/UCLA REU 2021 KG /results/Mar16-Apr16'\n",
        "df = read_json('document_data_mar16-apr16.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb-kx7Fh7NMq"
      },
      "source": [
        "with open('/content/drive/MyDrive/UCLA REU 2021 KG /results/paths_2.json', 'r') as f:\n",
        "  thread = f.readlines()[8]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71xO94KK-LyT"
      },
      "source": [
        "print(thread)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oojX5Piq8SrK"
      },
      "source": [
        "thread = ['1241466302398484480', '1241468184760184832', '1241469151085907968', '1241469473166462976', '1241469808463327232', '1241470575572221952', '1241471366253047808']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF9YCfTz775c"
      },
      "source": [
        "pd.set_option('max_columns', None)\n",
        "for t in thread: \n",
        "  #print(numpy.int64(t))\n",
        "  series = df.loc[df['tweet-id'] ==  numpy.int64(t)]\n",
        "  print(series['tweet-id'])\n",
        "  print(series.to_dict(orient = 'records'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz3_LhpeAFWM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv2itsL67Jsx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJoJaht9scN8"
      },
      "source": [
        "pd.set_option('max_rows', None)\n",
        "keywords = topic_df['keywords'].to_list()\n",
        "for i in range(len(keywords)):\n",
        "  print(keywords[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}